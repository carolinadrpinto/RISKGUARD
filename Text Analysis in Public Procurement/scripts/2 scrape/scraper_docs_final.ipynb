{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85f7b83",
   "metadata": {},
   "source": [
    "### Initial acessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d97a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carol\\Documents\\bolsa\\RISKGUARD\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install selenium webdriver-manager\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utilities.funcoes import extrair_ids\n",
    "from utilities.funcoes_scrape import extract_archives_under, download_pecas_flat, keep_only_key_docs, download_contract_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f86936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carol\\AppData\\Local\\Temp\\ipykernel_20408\\3211104189.py:1: DtypeWarning: Columns (18,19,23,24,25,26,27,28,29,30,31,32,33,34,35,38,39,40,41,43,44,52,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  contratos = pd.read_csv(\"../data/impic_data/contratos.csv\", sep=';')\n"
     ]
    }
   ],
   "source": [
    "contratos = pd.read_csv(\"../data/impic_data/contratos.csv\", sep=';')\n",
    "contratos_niclas = pd.read_parquet(r\"..\\data\\contracts_2009_2024.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62577cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colocar as datas relevantes para a análise em datetime\n",
    "contratos_niclas['close_date']=pd.to_datetime(contratos_niclas['close_date'], errors='coerce')\n",
    "contratos_niclas['signing_date'] = pd.to_datetime(contratos_niclas['signing_date'], errors='coerce')\n",
    "contratos['Data Celebração']=pd.to_datetime(contratos['Data Celebração'], errors='coerce')\n",
    "\n",
    "# extrair o id dos documentos\n",
    "contratos_niclas['ids_extraidos_docs'] = contratos_niclas['documents'].apply(extrair_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0e5df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver os contratos que já têm o link para as Peças do Procedimento válido\n",
    "data = []\n",
    "with open(\"../data/contracts_links_OT.jsonl\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "contracts_scraped = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821b48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contracts_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026906ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Para cada um destes casos há um tratamento diferente\n",
    "\n",
    "# contratos que têm link direto para download\n",
    "contracts_download = contracts_scraped[contracts_scraped['pecas_link'].str.contains('donwload', case=False, na=False, regex=True)]\n",
    "# contratos que têm link para o vortal\n",
    "contracts_vortal = contracts_scraped[contracts_scraped['pecas_link'].str.contains('vortal', case=False, na=False, regex=True)]\n",
    "# contratos que foram scraped até à data\n",
    "contratos_niclas_scraped = contratos_niclas[(contratos_niclas['contract_id'].isin(contracts_scraped['contract_id'].to_list()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a34e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contracts_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edcc1c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contracts_vortal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24751b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# objetivo é ver as pastas que já foram criadas e exluir estes ids da extração por download do pdf e do anuncio\n",
    "# vou deixar a extração com o vortal para o fim, por isso todos os que tiverem donwload têm de ser passados ainda pelo vortal\n",
    "# todos os que estiverem nesta list não sáo passados pelo download nem pela extraçao do anuncio e do pdf do contrato, mas têm de ser passados pelo vortal se for esse o caso\n",
    "ids_extraidos = []\n",
    "for item in os.listdir('../docs2'):\n",
    "    ids_extraidos.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bb573ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos os que têm pasta eu já tentei fazer o download se têm download e já fiz também anuncio e contrato\n",
    "# vou acrescentar aqui depois o do vortal, quando tiver tudo certo\n",
    "# ou então faço a extração com o caso do vortal noutro notebook\n",
    "not_extracted = contracts_scraped[~(contracts_scraped['contract_id'].apply(lambda x: str(x)).isin(ids_extraidos))]\n",
    "not_extracted_download = not_extracted[not_extracted['pecas_link'].str.contains('donwload', case=False, na=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50985c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_extracted_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc990c",
   "metadata": {},
   "source": [
    "### Caso vortal\n",
    "\n",
    "Ainda tenho de melhorar este caso - fica para mais tarde, primeiro desenvolver a pipeline que já tenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08839507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium webdriver-manager requests\n",
    "\n",
    "import re, time, unicodedata\n",
    "from pathlib import Path\n",
    "import urllib.parse as up\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# --- Selectors (adjust if your tenant’s DOM differs) ---\n",
    "VORTAL_DOC_NAME = \"span.VortalSpan[id*='spnDocumentName']\"\n",
    "VORTAL_DL_LINK  = \"a[id^='lnkDownloadLink']\"\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _norm(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\s_]+\", \" \", s).strip()  # turn \"_\" into spaces and collapse runs\n",
    "    return s\n",
    "\n",
    "def _want(name: str, type: str) -> bool:\n",
    "    t = _norm(name)\n",
    "    # keep: \"caderno ... encargos\" OR \"programa\" OR \"ce\"\n",
    "    if type=='caderno':\n",
    "        return (('caderno' in t and \"encargos\" in t) or (\"ce \" in t) and ('programa' not in t) and ('anexo' not in t))\n",
    "    elif type=='programa':\n",
    "        return('programa' in t)\n",
    "    else:\n",
    "        raise ValueError(f\"type inválido: {type!r}. Esperado: 'caderno' ou 'programa'.\")\n",
    "    \n",
    "\n",
    "def _allow_downloads(drv, out_dir: Path):\n",
    "    try:\n",
    "        drv.execute_cdp_cmd(\"Page.setDownloadBehavior\", {\"behavior\": \"allow\", \"downloadPath\": str(out_dir)})\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _accept_cookies_if_present(drv):\n",
    "    for how, sel in [\n",
    "        (By.ID, \"onetrust-accept-btn-handler\"),\n",
    "        (By.XPATH, \"//button[contains(., 'Aceitar')]\"),\n",
    "        (By.XPATH, \"//button[contains(., 'Accept')]\"),\n",
    "        (By.XPATH, \"//button[contains(., 'I Agree')]\"),\n",
    "        (By.XPATH, \"//div[contains(@class,'cookie')]//button\"),\n",
    "    ]:\n",
    "        try:\n",
    "            btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((how, sel)))\n",
    "            drv.execute_script(\"arguments[0].click();\", btn)\n",
    "            time.sleep(0.2)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def _open_documents_tab(drv):\n",
    "    for xp in (\n",
    "        \"//a[normalize-space()='Documentos']\", \"//button[normalize-space()='Documentos']\",\n",
    "        \"//a[normalize-space()='Documents']\",  \"//button[normalize-space()='Documents']\",\n",
    "        \"//div[contains(@class,'tab')]//a[contains(.,'Documentos')]\",\n",
    "        \"//div[contains(@class,'tab')]//a[contains(.,'Documents')]\",\n",
    "    ):\n",
    "        try:\n",
    "            el = drv.find_element(By.XPATH, xp)\n",
    "            drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "            time.sleep(0.1)\n",
    "            drv.execute_script(\"arguments[0].click();\", el)\n",
    "            time.sleep(0.2)\n",
    "            return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def _scroll_page(drv, steps=6, pause=0.25):\n",
    "    last_h = 0\n",
    "    for _ in range(steps):\n",
    "        drv.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        h = drv.execute_script(\"return document.body.scrollHeight\")\n",
    "        if h == last_h: break\n",
    "        last_h = h\n",
    "\n",
    "def _parse_onclick_concat(onclick: str):\n",
    "    \"\"\"Collapse '...' + '...' into one string, then extract query params for DownloadFile.\"\"\"\n",
    "    if not onclick: return None, None\n",
    "    s = re.sub(r\"'\\s*\\+\\s*'\", \"\", onclick)\n",
    "    s = s.replace(\"'+ '\", \"\").replace(\"' +\", \"\")\n",
    "    m = re.search(r\"DownloadFile\\?([^'\\\" ]+)\", s)\n",
    "    if not m: return None, None\n",
    "    qd = dict(up.parse_qsl(m.group(1)))\n",
    "    return qd.get(\"documentFileId\"), qd.get(\"mkey\")\n",
    "\n",
    "def _downloads_done(dirpath: Path) -> bool:\n",
    "    return not any(p.suffix == \".crdownload\" for p in dirpath.glob(\"*\"))\n",
    "\n",
    "# ---------- Main ----------\n",
    "def download_vortal_selected_via_browser(row,type: str, out_root: str, headless: bool = False, timeout: int = 60, tries: int = 3):\n",
    "    out_dir = Path(out_root) / str(row['id'])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_dir = Path(out_dir).resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    prefs = {\n",
    "        \"download.default_directory\": str(out_dir),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True,  # force saving PDF (no in-tab viewer)\n",
    "    }\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--lang=pt-PT\")\n",
    "    opts.add_argument(\"--disable-popup-blocking\")\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    try:\n",
    "        _allow_downloads(drv, out_dir)\n",
    "\n",
    "        # load + prep page (with retries for lazy load / banners / tabs)\n",
    "        for attempt in range(1, tries+1):\n",
    "            drv.get(row['link'])\n",
    "            WebDriverWait(drv, timeout).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "            _accept_cookies_if_present(drv)\n",
    "            _open_documents_tab(drv)\n",
    "            _scroll_page(drv)\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(drv, timeout).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, VORTAL_DOC_NAME)))\n",
    "                if drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "                    break\n",
    "            except Exception:\n",
    "                if attempt == tries:\n",
    "                    # dump debug\n",
    "                    (out_dir/\"vortal_debug.png\").write_bytes(drv.get_screenshot_as_png())\n",
    "                    (out_dir/\"vortal_debug.html\").write_text(drv.page_source, encoding=\"utf-8\")\n",
    "                    return {\"ok\": False, \"reason\": \"doc_names_not_found\",\n",
    "                            \"debug\": [str(out_dir/\"vortal_debug.png\"), str(out_dir/\"vortal_debug.html\")]}\n",
    "                time.sleep(1.5 * attempt)\n",
    "\n",
    "        # Map index suffix -> visible name\n",
    "        idx_to_name = {}\n",
    "        for s in drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "            sid = s.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", sid)\n",
    "            idx = m.group(1) if m else sid\n",
    "            idx_to_name[idx] = (s.text or \"\").strip()\n",
    "\n",
    "        # Collect anchors and select only the wanted ones\n",
    "        targets = []\n",
    "        for a in drv.find_elements(By.CSS_SELECTOR, VORTAL_DL_LINK):\n",
    "            aid = a.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", aid)\n",
    "            idx = m.group(1) if m else aid\n",
    "            name = idx_to_name.get(idx, \"\").strip()\n",
    "            if name and _want(name, type=type):\n",
    "                targets.append((name, a))\n",
    "\n",
    "        if not targets:\n",
    "            return {\"ok\": True, \"matched\": 0, \"files\": [], \"note\": \"no matching names\"}\n",
    "\n",
    "        # Build direct download URLs and navigate to them (keeps browser context → no 403)\n",
    "        base = f\"{up.urlparse(row['link']).scheme}://{up.urlparse(row['link']).netloc}\"\n",
    "        before = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "\n",
    "        for name, a in targets:\n",
    "            onclick = a.get_attribute(\"onclick\") or \"\"\n",
    "            doc_id, mkey = _parse_onclick_concat(onclick)\n",
    "            if doc_id and mkey:\n",
    "                dl_url = f\"{base}/PRODPublic/Tendering/OpportunityDetail/DownloadFile?documentFileId={doc_id}&mkey={mkey}\"\n",
    "                drv.get(dl_url)  # triggers true download\n",
    "            else:\n",
    "                # fallback: click\n",
    "                drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", a)\n",
    "                drv.execute_script(\"arguments[0].click();\", a)\n",
    "            time.sleep(0.6)\n",
    "\n",
    "        # wait for downloads to finish\n",
    "        t0 = time.time()\n",
    "        max_wait = max(30, 15 * len(targets))\n",
    "        while time.time() - t0 < max_wait:\n",
    "            time.sleep(0.5)\n",
    "            if _downloads_done(out_dir):\n",
    "                break\n",
    "\n",
    "        after = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "        new_files = sorted(list(after - before))\n",
    "        return {\"ok\": True, \"matched\": len(targets), \"files\": [str(out_dir / f) for f in new_files]}\n",
    "\n",
    "    finally:\n",
    "        drv.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55db8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keep your imports ---\n",
    "import json, html\n",
    "import re, time, unicodedata\n",
    "from pathlib import Path\n",
    "import urllib.parse as up\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ... keep your existing constants and helpers ...\n",
    "# i am going to match only a document each time\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\s_]+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _want(name: str, type: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heurística simples para 'caderno' ou 'programa'.\n",
    "    - 'caderno': contém 'caderno' e 'encargos' ou é 'ce' isolado; exclui 'programa' e 'anexo'\n",
    "    - 'programa': contém 'programa'\n",
    "    \"\"\"\n",
    "    t = _norm(name)\n",
    "    if type == 'caderno':\n",
    "        is_ce = re.search(r\"\\bce\\b\", t) is not None\n",
    "        ok = ('caderno' in t or 'encargos' in t or is_ce)\n",
    "        bad = ('programa' in t) or ('anexo' in t)\n",
    "        return ok and not bad\n",
    "    elif type == 'programa':\n",
    "        return ('programa' in t)\n",
    "    else:\n",
    "        raise ValueError(f\"type inválido: {type!r}. Esperado: 'caderno' ou 'programa'.\")\n",
    "\n",
    "def _parse_onclick_concat(onclick: str):\n",
    "    \"\"\"\n",
    "    Collapse string concatenations in onclick and extract query params for DownloadFile.\n",
    "    Returns (documentFileId, mkey) or (None, None).\n",
    "    \"\"\"\n",
    "    if not onclick:\n",
    "        return None, None\n",
    "    s = html.unescape(onclick)  # decode &amp;\n",
    "    s = re.sub(r\"'\\s*\\+\\s*'\", \"\", s)\n",
    "    s = s.replace(\"'+ '\", \"\").replace(\"' +\", \"\")\n",
    "    m = re.search(r\"DownloadFile\\?([^'\\\" ]+)\", s)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    qd = dict(up.parse_qsl(m.group(1)))\n",
    "    return qd.get(\"documentFileId\"), qd.get(\"mkey\")\n",
    "\n",
    "def _downloads_done(dirpath: Path) -> bool:\n",
    "    return not any(p.suffix == \".crdownload\" for p in dirpath.glob(\"*\"))\n",
    "\n",
    "def _load_ledger(out_dir: Path) -> set:\n",
    "    \"\"\"Keep a tiny JSON ledger of already-downloaded documentFileId values.\"\"\"\n",
    "    led = out_dir / \"downloaded.json\"\n",
    "    if led.exists():\n",
    "        try:\n",
    "            return set(json.loads(led.read_text(encoding=\"utf-8\")))\n",
    "        except Exception:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def _save_ledger(out_dir: Path, ids: set):\n",
    "    (out_dir / \"downloaded.json\").write_text(json.dumps(sorted(ids)), encoding=\"utf-8\")\n",
    "\n",
    "def download_vortal_selected_via_browser(row, type: str, out_root: str,\n",
    "                                         headless: bool = False, timeout: int = 60,\n",
    "                                         tries: int = 3, max_files: int = 1):\n",
    "    \"\"\"\n",
    "    Downloads up to `max_files` matching documents (default: 1) for this contract row.\n",
    "    Prevents re-downloads by keeping a ledger of documentFileIds in downloaded.json.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    VORTAL_DOC_NAME = \"span.VortalSpan[id*='spnDocumentName']\"\n",
    "    VORTAL_DL_LINK  = \"a[id^='lnkDownloadLink']\"\n",
    "\n",
    "    out_dir = Path(out_root) / str(row['id'])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_dir = Path(out_dir).resolve()\n",
    "\n",
    "    # Load ledger (already downloaded documentFileIds)\n",
    "    ledger = _load_ledger(out_dir)\n",
    "\n",
    "    prefs = {\n",
    "        \"download.default_directory\": str(out_dir),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "    }\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--lang=pt-PT\")\n",
    "    opts.add_argument(\"--disable-popup-blocking\")\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    try:\n",
    "        # Allow downloads via CDP (best-effort)\n",
    "        try:\n",
    "            drv.execute_cdp_cmd(\"Page.setDownloadBehavior\", {\"behavior\": \"allow\", \"downloadPath\": str(out_dir)})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # load + prep page (with retries)\n",
    "        for attempt in range(1, tries+1):\n",
    "            drv.get(row['link'])\n",
    "            WebDriverWait(drv, timeout).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "\n",
    "            # cookies\n",
    "            for how, sel in [\n",
    "                (By.ID, \"onetrust-accept-btn-handler\"),\n",
    "                (By.XPATH, \"//button[contains(., 'Aceitar')]\"),\n",
    "                (By.XPATH, \"//button[contains(., 'Accept')]\"),\n",
    "                (By.XPATH, \"//button[contains(., 'I Agree')]\"),\n",
    "                (By.XPATH, \"//div[contains(@class,'cookie')]//button\"),\n",
    "            ]:\n",
    "                try:\n",
    "                    btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((how, sel)))\n",
    "                    drv.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(0.2)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # open \"Documentos\" tab\n",
    "            opened = False\n",
    "            for xp in (\n",
    "                \"//a[normalize-space()='Documentos']\", \"//button[normalize-space()='Documentos']\",\n",
    "                \"//a[normalize-space()='Documents']\",  \"//button[normalize-space()='Documents']\",\n",
    "                \"//div[contains(@class,'tab')]//a[contains(.,'Documentos')]\",\n",
    "                \"//div[contains(@class,'tab')]//a[contains(.,'Documents')]\",\n",
    "            ):\n",
    "                try:\n",
    "                    el = drv.find_element(By.XPATH, xp)\n",
    "                    drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    time.sleep(0.1)\n",
    "                    drv.execute_script(\"arguments[0].click();\", el)\n",
    "                    time.sleep(0.2)\n",
    "                    opened = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # scroll to load\n",
    "            last_h = 0\n",
    "            for _ in range(6):\n",
    "                drv.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.25)\n",
    "                h = drv.execute_script(\"return document.body.scrollHeight\")\n",
    "                if h == last_h: break\n",
    "                last_h = h\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(drv, timeout).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, VORTAL_DOC_NAME)))\n",
    "                if drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "                    break\n",
    "            except Exception:\n",
    "                if attempt == tries:\n",
    "                    (out_dir/\"vortal_debug.png\").write_bytes(drv.get_screenshot_as_png())\n",
    "                    (out_dir/\"vortal_debug.html\").write_text(drv.page_source, encoding=\"utf-8\")\n",
    "                    return {\"ok\": False, \"reason\": \"doc_names_not_found\",\n",
    "                            \"debug\": [str(out_dir/'vortal_debug.png'), str(out_dir/'vortal_debug.html')]}\n",
    "                time.sleep(1.5 * attempt)\n",
    "\n",
    "        # Map index suffix -> visible name\n",
    "        idx_to_name = {}\n",
    "        for s in drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "            sid = s.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", sid)\n",
    "            idx = m.group(1) if m else sid\n",
    "            idx_to_name[idx] = (s.text or \"\").strip()\n",
    "\n",
    "        # Collect anchors and pair with their doc ids via onclick\n",
    "        candidates = []\n",
    "        for a in drv.find_elements(By.CSS_SELECTOR, VORTAL_DL_LINK):\n",
    "            aid = a.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", aid)\n",
    "            idx = m.group(1) if m else aid\n",
    "            name = idx_to_name.get(idx, \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            if not _want(name, type=type):\n",
    "                continue\n",
    "            onclick = a.get_attribute(\"onclick\") or \"\"\n",
    "            doc_id, mkey = _parse_onclick_concat(onclick)\n",
    "            candidates.append({\"name\": name, \"a\": a, \"doc_id\": doc_id, \"mkey\": mkey})\n",
    "\n",
    "        # Remove ones already downloaded (using the ledger)\n",
    "        targets = [c for c in candidates if c[\"doc_id\"] and c[\"doc_id\"] not in ledger]\n",
    "\n",
    "        if not targets:\n",
    "            return {\"ok\": True, \"matched\": 0, \"files\": [], \"note\": \"no new matching names (all already downloaded or none match)\"}\n",
    "\n",
    "        # First-match only (or up to max_files)\n",
    "        targets = targets[:max_files]\n",
    "\n",
    "        # Prepare before/after listing\n",
    "        before = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "\n",
    "        # Build direct URL when possible, else click\n",
    "        base = f\"{up.urlparse(row['link']).scheme}://{up.urlparse(row['link']).netloc}\"\n",
    "        for c in targets:\n",
    "            if c[\"doc_id\"] and c[\"mkey\"]:\n",
    "                dl_url = f\"{base}/PRODPublic/Tendering/OpportunityDetail/DownloadFile?documentFileId={c['doc_id']}&mkey={c['mkey']}\"\n",
    "                drv.get(dl_url)\n",
    "            else:\n",
    "                drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", c[\"a\"])\n",
    "                drv.execute_script(\"arguments[0].click();\", c[\"a\"])\n",
    "            time.sleep(0.7)\n",
    "\n",
    "        # wait for downloads to finish\n",
    "        t0 = time.time()\n",
    "        max_wait = max(30, 15 * len(targets))\n",
    "        while time.time() - t0 < max_wait:\n",
    "            time.sleep(0.5)\n",
    "            if _downloads_done(out_dir):\n",
    "                break\n",
    "\n",
    "        after = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "        new_files = sorted(list(after - before))\n",
    "\n",
    "        # Update ledger with the doc_ids we attempted (only if new files appeared)\n",
    "        for c in targets:\n",
    "            if c[\"doc_id\"]:\n",
    "                ledger.add(c[\"doc_id\"])\n",
    "        _save_ledger(out_dir, ledger)\n",
    "\n",
    "        return {\"ok\": True, \"matched\": len(targets), \"files\": [str(out_dir / f) for f in new_files]}\n",
    "\n",
    "    finally:\n",
    "        drv.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "536e19cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                {'ok': True, 'matched': 1, 'files': []}\n",
       "2      {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "3      {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "4      {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "6      {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "                             ...                        \n",
       "792    {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "794    {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "795    {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "797    {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "799    {'ok': True, 'matched': 1, 'files': ['C:\\Users...\n",
       "Length: 387, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tentar extrair o caderno de encargos\n",
    "# problema - muitas vezes vem como CE, acho que já resolvi o problema, não sei se está à prova de bala\n",
    "contracts_vortal.apply(download_vortal_selected_via_browser, axis=1, type = 'caderno', headless=False, timeout=60, out_root='../docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ef8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_vortal.apply(download_vortal_selected_via_browser, axis=1, type = 'programa', headless=False, timeout=60, out_root='../docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8898807",
   "metadata": {},
   "source": [
    "### Caso download direto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de420c6e",
   "metadata": {},
   "source": [
    "#### Fazer o download da pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67dbd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = contracts_download.apply(download_pecas_flat, axis=1, out_root=\"../docs2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48302436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download peças do procedimento folders\n",
    "saved2 = not_extracted_download.apply(download_pecas_flat, axis=1, out_root=\"../docs2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e5c19",
   "metadata": {},
   "source": [
    "#### Unzip Folders that were ziped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1f042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 107 archives\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# unzip downloaded folders inside the main folder\n",
    "\n",
    "summary = extract_archives_under(Path(\"../docs2\"), max_zip_depth=4, recursive=False)\n",
    "print(\"Processed:\", len(summary[\"processed\"]), \"archives\")\n",
    "print(\"Errors:\", len(summary[\"errors\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcef21",
   "metadata": {},
   "source": [
    "#### Filtrar só pelos documentos que me interessam - CE e programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e06e8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "\n",
    "folder_path = Path(\"../docs2\")\n",
    "for p in folder_path.iterdir():\n",
    "    if p.is_dir():                      # only run on directories\n",
    "        result = keep_only_key_docs(p, delete_nonimportant=True)\n",
    "        resultados.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92aec98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['..\\\\docs2\\\\9684721\\\\1_2.1._CadernoEncargos_e_anexos_VA_2023.pdf', '..\\\\docs2\\\\9684721\\\\3_DEUCP_espd-request.pdf', '..\\\\docs2\\\\9684721\\\\Anuncio13919_2022_DRE210_31out2022_DRE.pdf']),\n",
       "       list(['..\\\\docs2\\\\9689626\\\\Anexo IV - declaraç╞o de preços unitários.xlsx', '..\\\\docs2\\\\9689626\\\\anuncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9692428\\\\2_2022_081CP_PP_Reagentes_an_lise_de_guas.pdf', '..\\\\docs2\\\\9692428\\\\CCP2021A0141953244.pdf']),\n",
       "       list(['..\\\\docs2\\\\9697589\\\\2_2022_081CP_PP_Reagentes_an_lise_de_guas.pdf', '..\\\\docs2\\\\9697589\\\\CCP2021A0141953244.pdf']),\n",
       "       list(['..\\\\docs2\\\\9698256\\\\2_2022_081CP_PP_Reagentes_an_lise_de_guas.pdf', '..\\\\docs2\\\\9698256\\\\CCP2021A0141953244.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709215\\\\1_espd-request.xml', '..\\\\docs2\\\\9709215\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709215\\\\Lista.pdf', '..\\\\docs2\\\\9709215\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709256\\\\1_espd-request.xml', '..\\\\docs2\\\\9709256\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709256\\\\Lista.pdf', '..\\\\docs2\\\\9709256\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709314\\\\1_espd-request.xml', '..\\\\docs2\\\\9709314\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709314\\\\Lista.pdf', '..\\\\docs2\\\\9709314\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709355\\\\1_espd-request.xml', '..\\\\docs2\\\\9709355\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709355\\\\Lista.pdf', '..\\\\docs2\\\\9709355\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709527\\\\1_espd-request.xml', '..\\\\docs2\\\\9709527\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709527\\\\Lista.pdf', '..\\\\docs2\\\\9709527\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709740\\\\1_espd-request.xml', '..\\\\docs2\\\\9709740\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709740\\\\Lista.pdf', '..\\\\docs2\\\\9709740\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9709941\\\\1_espd-request.xml', '..\\\\docs2\\\\9709941\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9709941\\\\Lista.pdf', '..\\\\docs2\\\\9709941\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9710006\\\\1_espd-request.xml', '..\\\\docs2\\\\9710006\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9710006\\\\Lista.pdf', '..\\\\docs2\\\\9710006\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9710058\\\\1_espd-request.xml', '..\\\\docs2\\\\9710058\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9710058\\\\Lista.pdf', '..\\\\docs2\\\\9710058\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9710105\\\\1_espd-request.xml', '..\\\\docs2\\\\9710105\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9710105\\\\Lista.pdf', '..\\\\docs2\\\\9710105\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9710484\\\\1_espd-request.xml', '..\\\\docs2\\\\9710484\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9710484\\\\Lista.pdf', '..\\\\docs2\\\\9710484\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9710570\\\\1_espd-request.xml', '..\\\\docs2\\\\9710570\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9710570\\\\Lista.pdf', '..\\\\docs2\\\\9710570\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9711649\\\\1_espd-request.xml', '..\\\\docs2\\\\9711649\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9711649\\\\Lista.pdf', '..\\\\docs2\\\\9711649\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9715612\\\\Anuncio 12167_2022.pdf', '..\\\\docs2\\\\9715612\\\\Lista.pdf']),\n",
       "       list(['..\\\\docs2\\\\9715637\\\\Anuncio 12167_2022.pdf', '..\\\\docs2\\\\9715637\\\\Lista.pdf']),\n",
       "       list(['..\\\\docs2\\\\9722182\\\\1_espd-request.xml', '..\\\\docs2\\\\9722182\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9722182\\\\Lista.pdf', '..\\\\docs2\\\\9722182\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9739666\\\\Anúncio do DRE (1).pdf', '..\\\\docs2\\\\9739666\\\\Anúncio do DRE.pdf', '..\\\\docs2\\\\9739666\\\\Lista (1).pdf', '..\\\\docs2\\\\9739666\\\\Lista.pdf']),\n",
       "       list(['..\\\\docs2\\\\9753644\\\\1_espd-request.xml', '..\\\\docs2\\\\9753644\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9753644\\\\Lista.pdf', '..\\\\docs2\\\\9753644\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9798452\\\\Anúncio DRE.pdf']),\n",
       "       list(['..\\\\docs2\\\\9798468\\\\Anúncio DRE.pdf']),\n",
       "       list(['..\\\\docs2\\\\9806729\\\\Anúncio DR.pdf', '..\\\\docs2\\\\9806729\\\\espd-request.pdf', '..\\\\docs2\\\\9806729\\\\espd-request.xml', '..\\\\docs2\\\\9806729\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9814111\\\\Anuncio DR n.º 13147 2022.pdf', '..\\\\docs2\\\\9814111\\\\Lista.pdf']),\n",
       "       list(['..\\\\docs2\\\\9826082\\\\1_MDJ_AA_Av-Liberdade.pdf', '..\\\\docs2\\\\9826082\\\\2022_03_23_MDJ_EST_Av_Liberdade.pdf', '..\\\\docs2\\\\9826082\\\\2022_03_23_RI_PAV_signed.pdf', '..\\\\docs2\\\\9826082\\\\2022_11_04_AP_MDJ_signed.pdf', '..\\\\docs2\\\\9826082\\\\2022_11_07_FaseamentoConstrutivo.dwfx', '..\\\\docs2\\\\9826082\\\\2022_11_10_Estabilidade_Av_Liberdade_V02.dwfx', '..\\\\docs2\\\\9826082\\\\2022_11_11_MDJ_Pavimentos_signed.pdf', '..\\\\docs2\\\\9826082\\\\2022_11_14_AP.dwfx', '..\\\\docs2\\\\9826082\\\\2022_11_14_PAV.dwfx', '..\\\\docs2\\\\9826082\\\\22001_AA (1).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA (1).pdf', '..\\\\docs2\\\\9826082\\\\22001_AA (2).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA (2).pdf', '..\\\\docs2\\\\9826082\\\\22001_AA (3).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA (3).pdf', '..\\\\docs2\\\\9826082\\\\22001_AA.DWG', '..\\\\docs2\\\\9826082\\\\22001_AA.pdf', '..\\\\docs2\\\\9826082\\\\22001_AA_ (1).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA_ (2).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA_ (3).DWG', '..\\\\docs2\\\\9826082\\\\22001_AA_.DWG', '..\\\\docs2\\\\9826082\\\\22001_AR (1).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR (1).pdf', '..\\\\docs2\\\\9826082\\\\22001_AR (2).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR (2).pdf', '..\\\\docs2\\\\9826082\\\\22001_AR (3).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR (3).pdf', '..\\\\docs2\\\\9826082\\\\22001_AR.DWG', '..\\\\docs2\\\\9826082\\\\22001_AR.pdf', '..\\\\docs2\\\\9826082\\\\22001_AR_ (1).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR_ (2).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR_ (3).DWG', '..\\\\docs2\\\\9826082\\\\22001_AR_.DWG', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030a (1).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030a (2).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030a (3).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030a.pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030b (1).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030b (2).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030b (3).pdf', '..\\\\docs2\\\\9826082\\\\22F_PLANT_005030b.pdf', '..\\\\docs2\\\\9826082\\\\2_PRO50_00_AA_Desinf-lavagem-condutas-novas.pdf', '..\\\\docs2\\\\9826082\\\\3_PRO51_00_AA_Ensaio-Estanquidade-Condutas.pdf', '..\\\\docs2\\\\9826082\\\\415900732.pdf', '..\\\\docs2\\\\9826082\\\\415983297.pdf', '..\\\\docs2\\\\9826082\\\\acinGov-ListaArtigos.xls', '..\\\\docs2\\\\9826082\\\\AGR_AA_AV LIBERDADE_Projecto_v1.dwfx', '..\\\\docs2\\\\9826082\\\\BRG-AVENIDAS (1).dwg', '..\\\\docs2\\\\9826082\\\\BRG-AVENIDAS (2).dwg', '..\\\\docs2\\\\9826082\\\\BRG-AVENIDAS (3).dwg', '..\\\\docs2\\\\9826082\\\\BRG-AVENIDAS.dwg', '..\\\\docs2\\\\9826082\\\\CABRG22001 (1).pdf', '..\\\\docs2\\\\9826082\\\\CABRG22001 (2).pdf', '..\\\\docs2\\\\9826082\\\\CABRG22001 (3).pdf', '..\\\\docs2\\\\9826082\\\\CABRG22001.pdf', '..\\\\docs2\\\\9826082\\\\Decisão sobre E.Omissões.pdf', '..\\\\docs2\\\\9826082\\\\Desenho Contentores Enterrados.pdf', '..\\\\docs2\\\\9826082\\\\Desenho_Contentores_Superficie.pdf', '..\\\\docs2\\\\9826082\\\\E-REDES_parecer.pdf', '..\\\\docs2\\\\9826082\\\\EJPA-RequalificacaoAvLiberdade_PSemaforizacao-MD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA-RequalificacaoAvLiberdade_PSemaforizacao_PD.dwfx', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_MobiliarioUrbano-MD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_MobiliarioUrbano_PD.dwfx', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_MobiliarioUrbano_PD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PArq-MD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PArq_PD.dwfx', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PArq_PD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PArq_PD_07112022.dwl', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PArq_PD_07112022.dwl2', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PSinalizacao-MD.pdf', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PSinalizacao_PD.dwfx', '..\\\\docs2\\\\9826082\\\\EJPA_Av-Liberdade_PSinalizacao_PD.pdf', '..\\\\docs2\\\\9826082\\\\Faseamento-Construtivo_Av-Liberdade_07112022_signed.pdf', '..\\\\docs2\\\\9826082\\\\FELET.pdf', '..\\\\docs2\\\\9826082\\\\Ficha_Zebra_ES.pdf', '..\\\\docs2\\\\9826082\\\\FIP.pdf', '..\\\\docs2\\\\9826082\\\\Indice_AP.pdf', '..\\\\docs2\\\\9826082\\\\Indice_Est.pdf', '..\\\\docs2\\\\9826082\\\\Indice_Fas_Cons.pdf', '..\\\\docs2\\\\9826082\\\\Indice_Pav.pdf', '..\\\\docs2\\\\9826082\\\\INF_CSP.pdf', '..\\\\docs2\\\\9826082\\\\Infraestruturas_RedeSemaforica-CCTV.pdf', '..\\\\docs2\\\\9826082\\\\LIST_ESCL_EO_05122022.pdf', '..\\\\docs2\\\\9826082\\\\LIST_ESCL_EO_05122022.xlsx', '..\\\\docs2\\\\9826082\\\\Lista.pdf', '..\\\\docs2\\\\9826082\\\\MDAE.pdf', '..\\\\docs2\\\\9826082\\\\MMED-MQT-MO_REV01_05122022.xlsx', '..\\\\docs2\\\\9826082\\\\MMED-MQT.xlsx', '..\\\\docs2\\\\9826082\\\\MMED_07112022.pdf', '..\\\\docs2\\\\9826082\\\\MMED_REV01_05122022.pdf', '..\\\\docs2\\\\9826082\\\\MQT_07112022.pdf', '..\\\\docs2\\\\9826082\\\\MQT_REV01_05122022.pdf', '..\\\\docs2\\\\9826082\\\\PADE.dwfx', '..\\\\docs2\\\\9826082\\\\PPGRCD_Av-Liberdade_27102022_signed.pdf', '..\\\\docs2\\\\9826082\\\\PSS-P.pdf', '..\\\\docs2\\\\9826082\\\\RelatórioEnsaios Av. Liberdade.pdf', '..\\\\docs2\\\\9826082\\\\Thumbs.db']),\n",
       "       list(['..\\\\docs2\\\\9829838\\\\Anúncio n.º8124_2022.pdf', '..\\\\docs2\\\\9829838\\\\espd-request.pdf', '..\\\\docs2\\\\9829838\\\\espd-request.xml', '..\\\\docs2\\\\9829838\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9830567\\\\Anúncio n.º8124_2022.pdf', '..\\\\docs2\\\\9830567\\\\espd-request.pdf', '..\\\\docs2\\\\9830567\\\\espd-request.xml', '..\\\\docs2\\\\9830567\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9832925\\\\Anúncio prorrogação publicado DRE.pdf', '..\\\\docs2\\\\9832925\\\\Anúncio Publicado DRE.pdf', '..\\\\docs2\\\\9832925\\\\espd-request.pdf', '..\\\\docs2\\\\9832925\\\\espd-request.xml', '..\\\\docs2\\\\9832925\\\\Lista.pdf', '..\\\\docs2\\\\9832925\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9833873\\\\Anúncio prorrogação publicado DRE.pdf', '..\\\\docs2\\\\9833873\\\\Anúncio Publicado DRE.pdf', '..\\\\docs2\\\\9833873\\\\espd-request.pdf', '..\\\\docs2\\\\9833873\\\\espd-request.xml', '..\\\\docs2\\\\9833873\\\\Lista.pdf', '..\\\\docs2\\\\9833873\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9834241\\\\Anúncio prorrogação publicado DRE.pdf', '..\\\\docs2\\\\9834241\\\\Anúncio Publicado DRE.pdf', '..\\\\docs2\\\\9834241\\\\espd-request.pdf', '..\\\\docs2\\\\9834241\\\\espd-request.xml', '..\\\\docs2\\\\9834241\\\\Lista.pdf', '..\\\\docs2\\\\9834241\\\\README.txt']),\n",
       "       list(['..\\\\docs2\\\\9840931\\\\DRE.pdf', '..\\\\docs2\\\\9840931\\\\espd-request.pdf', '..\\\\docs2\\\\9840931\\\\espd-request.xml', '..\\\\docs2\\\\9840931\\\\README.txt', '..\\\\docs2\\\\9840931\\\\Thumbs.db']),\n",
       "       list(['..\\\\docs2\\\\9849807\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9849807\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9849807\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9849829\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9849829\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9849829\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9853246\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9853246\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9853246\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9853262\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9853262\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9853262\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9865565\\\\1_espd-request.xml', '..\\\\docs2\\\\9865565\\\\Anuncio 14513_2022.pdf', '..\\\\docs2\\\\9865565\\\\Lista.pdf', '..\\\\docs2\\\\9865565\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9889790\\\\CCP2021A0142019671.pdf']),\n",
       "       list(['..\\\\docs2\\\\9893387\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9893387\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9893387\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9893397\\\\2022_136CP BARRAGENS - CE_retificado.pdf', '..\\\\docs2\\\\9893397\\\\2022_136CP BARRAGENS - PP_retificado.pdf', '..\\\\docs2\\\\9893397\\\\CCP2021A0141997692.pdf']),\n",
       "       list(['..\\\\docs2\\\\9900246\\\\DRE_serviços.pdf']),\n",
       "       list(['..\\\\docs2\\\\9900405\\\\DRE_serviços.pdf']),\n",
       "       list(['..\\\\docs2\\\\9929739\\\\DR 14197_2021.pdf']),\n",
       "       list(['..\\\\docs2\\\\9931604\\\\1_espd-request.xml', '..\\\\docs2\\\\9931604\\\\Anuncio 16259_2022.pdf', '..\\\\docs2\\\\9931604\\\\Lista.pdf', '..\\\\docs2\\\\9931604\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9931623\\\\1_espd-request.xml', '..\\\\docs2\\\\9931623\\\\Anuncio 16259_2022.pdf', '..\\\\docs2\\\\9931623\\\\Lista.pdf', '..\\\\docs2\\\\9931623\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9931714\\\\1_espd-request.xml', '..\\\\docs2\\\\9931714\\\\Anuncio 16259_2022.pdf', '..\\\\docs2\\\\9931714\\\\Lista.pdf', '..\\\\docs2\\\\9931714\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9931877\\\\1_espd-request.xml', '..\\\\docs2\\\\9931877\\\\Anuncio 16259_2022.pdf', '..\\\\docs2\\\\9931877\\\\Lista.pdf', '..\\\\docs2\\\\9931877\\\\Minuta do anúncio.pdf']),\n",
       "       list(['..\\\\docs2\\\\9932396\\\\1_espd-request.xml', '..\\\\docs2\\\\9932396\\\\Anuncio 16259_2022.pdf', '..\\\\docs2\\\\9932396\\\\Lista.pdf', '..\\\\docs2\\\\9932396\\\\Minuta do anúncio.pdf'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(resultados)['removed'].values[250:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b966e",
   "metadata": {},
   "source": [
    "### Extrair o pdf do anúncio e do contrato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a141a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge o ids extraidos com o código dos scrape e depois correr na função acima\n",
    "\n",
    "contracts_anun_contr = contracts_scraped.merge(contratos_niclas[['ids_extraidos_docs', 'contract_id']], how='left', left_on = 'contract_id', right_on='contract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergo do que quero extrair com o dataset do niclas porque tem o doc ID\n",
    "contracts_anun_contr = not_extracted.merge(contratos_niclas[['ids_extraidos_docs', 'contract_id']], how='left', left_on = 'contract_id', right_on='contract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_anunc_contrato = [download_contract_pdfs(row, out_folder = \"../docs2\") for _, row in contracts_anun_contr.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1b909",
   "metadata": {},
   "source": [
    "### Código reciclado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pecas_link_base(contract_id: int, headless: bool = False, timeout: int = 60, max_tries: int = 5):\n",
    "    \"\"\"\n",
    "    Vai à página do contrato, tenta ler o link em:\n",
    "      td[data-title=\"Peças do procedimento\"] a\n",
    "    Se não houver, devolve tem_link=False.\n",
    "    Recarrega a página com backoff quando a tabela não carrega.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.base.gov.pt/Base4/pt/detalhe/?type=contratos&id={contract_id}\"\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless: opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1366,900\")\n",
    "\n",
    "    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    try:\n",
    "        backoff = 2.0\n",
    "        for attempt in range(1, max_tries + 1):\n",
    "            drv.get(url)\n",
    "\n",
    "            # espera o contêiner onde a tabela é injectada via JS\n",
    "            try:\n",
    "                WebDriverWait(drv, timeout).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"no-more-tables-mx767\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                # se nem o container aparece, faz refresh com backoff\n",
    "                time.sleep(backoff + random.uniform(0, 1.5))\n",
    "                backoff *= 1.8\n",
    "                continue\n",
    "\n",
    "            # tenta apanhar diretamente o link das peças\n",
    "            try:\n",
    "                a = WebDriverWait(drv, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'td[data-title=\"Peças do procedimento\"] a'))\n",
    "                )\n",
    "                href = (a.get_attribute(\"href\") or \"\").strip()\n",
    "                return {\"tem_link\": bool(href), \"link\": href or None}\n",
    "            except Exception:\n",
    "                # se a célula existe mas sem <a>, significa \"sem link\"\n",
    "                tds = drv.find_elements(By.CSS_SELECTOR, 'td[data-title=\"Peças do procedimento\"]')\n",
    "                if tds:\n",
    "                    txt = (tds[0].text or \"\").strip()\n",
    "                    if not txt or txt == \"-\":\n",
    "                        return {\"tem_link\": False, \"link\": None}\n",
    "\n",
    "            # não conseguiu agora — espera e tenta outra vez (provável rate-limit / servidor lento)\n",
    "            time.sleep(backoff + random.uniform(0.5, 2.0))\n",
    "            backoff *= 1.8\n",
    "\n",
    "        # esgotou tentativas\n",
    "        return {\"tem_link\": False, \"link\": None}\n",
    "    finally:\n",
    "        drv.quit()\n",
    "\n",
    "\n",
    "# GUARDAR UM FICHEIRO JSONL\n",
    "\n",
    "def save_result(contract_id: int, result: dict, outfile: Path):\n",
    "    \"\"\"\n",
    "    Acrescenta uma linha ao ficheiro JSONL com:\n",
    "    {\"id\": ..., \"tem_link\": ..., \"link\": ...}\n",
    "    \"\"\"\n",
    "    data = {\"id\": contract_id, **result}\n",
    "    with outfile.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "        print('Guardado:', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keep your imports ---\n",
    "import json, html\n",
    "import re, time, unicodedata\n",
    "from pathlib import Path\n",
    "import urllib.parse as up\n",
    "\n",
    "# ... keep your existing constants and helpers ...\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[\\s_]+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _want(name: str, type: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heurística simples para 'caderno' ou 'programa'.\n",
    "    - 'caderno': contém 'caderno' e 'encargos' ou é 'ce' isolado; exclui 'programa' e 'anexo'\n",
    "    - 'programa': contém 'programa'\n",
    "    \"\"\"\n",
    "    t = _norm(name)\n",
    "    if type == 'caderno':\n",
    "        is_ce = re.search(r\"\\bce\\b\", t) is not None\n",
    "        ok = (('caderno' in t and 'encargos' in t) or is_ce)\n",
    "        bad = ('programa' in t) or ('anexo' in t)\n",
    "        return ok and not bad\n",
    "    elif type == 'programa':\n",
    "        return ('programa' in t)\n",
    "    else:\n",
    "        raise ValueError(f\"type inválido: {type!r}. Esperado: 'caderno' ou 'programa'.\")\n",
    "\n",
    "def _parse_onclick_concat(onclick: str):\n",
    "    \"\"\"\n",
    "    Collapse string concatenations in onclick and extract query params for DownloadFile.\n",
    "    Returns (documentFileId, mkey) or (None, None).\n",
    "    \"\"\"\n",
    "    if not onclick:\n",
    "        return None, None\n",
    "    s = html.unescape(onclick)  # decode &amp;\n",
    "    s = re.sub(r\"'\\s*\\+\\s*'\", \"\", s)\n",
    "    s = s.replace(\"'+ '\", \"\").replace(\"' +\", \"\")\n",
    "    m = re.search(r\"DownloadFile\\?([^'\\\" ]+)\", s)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    qd = dict(up.parse_qsl(m.group(1)))\n",
    "    return qd.get(\"documentFileId\"), qd.get(\"mkey\")\n",
    "\n",
    "def _downloads_done(dirpath: Path) -> bool:\n",
    "    return not any(p.suffix == \".crdownload\" for p in dirpath.glob(\"*\"))\n",
    "\n",
    "def _load_ledger(out_dir: Path) -> set:\n",
    "    \"\"\"Keep a tiny JSON ledger of already-downloaded documentFileId values.\"\"\"\n",
    "    led = out_dir / \"downloaded.json\"\n",
    "    if led.exists():\n",
    "        try:\n",
    "            return set(json.loads(led.read_text(encoding=\"utf-8\")))\n",
    "        except Exception:\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def _save_ledger(out_dir: Path, ids: set):\n",
    "    (out_dir / \"downloaded.json\").write_text(json.dumps(sorted(ids)), encoding=\"utf-8\")\n",
    "\n",
    "def download_vortal_selected_via_browser(row, type: str, out_root: str,\n",
    "                                         headless: bool = False, timeout: int = 60,\n",
    "                                         tries: int = 3, max_files: int = 1):\n",
    "    \"\"\"\n",
    "    Downloads up to `max_files` matching documents (default: 1) for this contract row.\n",
    "    Prevents re-downloads by keeping a ledger of documentFileIds in downloaded.json.\n",
    "    \"\"\"\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "    VORTAL_DOC_NAME = \"span.VortalSpan[id*='spnDocumentName']\"\n",
    "    VORTAL_DL_LINK  = \"a[id^='lnkDownloadLink']\"\n",
    "\n",
    "    out_dir = Path(out_root) / str(row['id'])\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_dir = Path(out_dir).resolve()\n",
    "\n",
    "    # Load ledger (already downloaded documentFileIds)\n",
    "    ledger = _load_ledger(out_dir)\n",
    "\n",
    "    prefs = {\n",
    "        \"download.default_directory\": str(out_dir),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "    }\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--lang=pt-PT\")\n",
    "    opts.add_argument(\"--disable-popup-blocking\")\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    drv = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    try:\n",
    "        # Allow downloads via CDP (best-effort)\n",
    "        try:\n",
    "            drv.execute_cdp_cmd(\"Page.setDownloadBehavior\", {\"behavior\": \"allow\", \"downloadPath\": str(out_dir)})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # load + prep page (with retries)\n",
    "        for attempt in range(1, tries+1):\n",
    "            drv.get(row['link'])\n",
    "            WebDriverWait(drv, timeout).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "\n",
    "            # cookies\n",
    "            for how, sel in [\n",
    "                (By.ID, \"onetrust-accept-btn-handler\"),\n",
    "                (By.XPATH, \"//button[contains(., 'Aceitar')]\"),\n",
    "                (By.XPATH, \"//button[contains(., 'Accept')]\"),\n",
    "                (By.XPATH, \"//button[contains(., 'I Agree')]\"),\n",
    "                (By.XPATH, \"//div[contains(@class,'cookie')]//button\"),\n",
    "            ]:\n",
    "                try:\n",
    "                    btn = WebDriverWait(drv, 3).until(EC.element_to_be_clickable((how, sel)))\n",
    "                    drv.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(0.2)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            # open \"Documentos\" tab\n",
    "            opened = False\n",
    "            for xp in (\n",
    "                \"//a[normalize-space()='Documentos']\", \"//button[normalize-space()='Documentos']\",\n",
    "                \"//a[normalize-space()='Documents']\",  \"//button[normalize-space()='Documents']\",\n",
    "                \"//div[contains(@class,'tab')]//a[contains(.,'Documentos')]\",\n",
    "                \"//div[contains(@class,'tab')]//a[contains(.,'Documents')]\",\n",
    "            ):\n",
    "                try:\n",
    "                    el = drv.find_element(By.XPATH, xp)\n",
    "                    drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    time.sleep(0.1)\n",
    "                    drv.execute_script(\"arguments[0].click();\", el)\n",
    "                    time.sleep(0.2)\n",
    "                    opened = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # scroll to load\n",
    "            last_h = 0\n",
    "            for _ in range(6):\n",
    "                drv.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.25)\n",
    "                h = drv.execute_script(\"return document.body.scrollHeight\")\n",
    "                if h == last_h: break\n",
    "                last_h = h\n",
    "\n",
    "            try:\n",
    "                WebDriverWait(drv, timeout).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, VORTAL_DOC_NAME)))\n",
    "                if drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "                    break\n",
    "            except Exception:\n",
    "                if attempt == tries:\n",
    "                    (out_dir/\"vortal_debug.png\").write_bytes(drv.get_screenshot_as_png())\n",
    "                    (out_dir/\"vortal_debug.html\").write_text(drv.page_source, encoding=\"utf-8\")\n",
    "                    return {\"ok\": False, \"reason\": \"doc_names_not_found\",\n",
    "                            \"debug\": [str(out_dir/'vortal_debug.png'), str(out_dir/'vortal_debug.html')]}\n",
    "                time.sleep(1.5 * attempt)\n",
    "\n",
    "        # Map index suffix -> visible name\n",
    "        idx_to_name = {}\n",
    "        for s in drv.find_elements(By.CSS_SELECTOR, VORTAL_DOC_NAME):\n",
    "            sid = s.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", sid)\n",
    "            idx = m.group(1) if m else sid\n",
    "            idx_to_name[idx] = (s.text or \"\").strip()\n",
    "\n",
    "        # Collect anchors and pair with their doc ids via onclick\n",
    "        candidates = []\n",
    "        for a in drv.find_elements(By.CSS_SELECTOR, VORTAL_DL_LINK):\n",
    "            aid = a.get_attribute(\"id\") or \"\"\n",
    "            m = re.search(r\"_(\\d+)$\", aid)\n",
    "            idx = m.group(1) if m else aid\n",
    "            name = idx_to_name.get(idx, \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "            if not _want(name, type=type):\n",
    "                continue\n",
    "            onclick = a.get_attribute(\"onclick\") or \"\"\n",
    "            doc_id, mkey = _parse_onclick_concat(onclick)\n",
    "            candidates.append({\"name\": name, \"a\": a, \"doc_id\": doc_id, \"mkey\": mkey})\n",
    "\n",
    "        # Remove ones already downloaded (using the ledger)\n",
    "        targets = [c for c in candidates if c[\"doc_id\"] and c[\"doc_id\"] not in ledger]\n",
    "\n",
    "        if not targets:\n",
    "            return {\"ok\": True, \"matched\": 0, \"files\": [], \"note\": \"no new matching names (all already downloaded or none match)\"}\n",
    "\n",
    "        # First-match only (or up to max_files)\n",
    "        targets = targets[:max_files]\n",
    "\n",
    "        # Prepare before/after listing\n",
    "        before = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "\n",
    "        # Build direct URL when possible, else click\n",
    "        base = f\"{up.urlparse(row['link']).scheme}://{up.urlparse(row['link']).netloc}\"\n",
    "        for c in targets:\n",
    "            if c[\"doc_id\"] and c[\"mkey\"]:\n",
    "                dl_url = f\"{base}/PRODPublic/Tendering/OpportunityDetail/DownloadFile?documentFileId={c['doc_id']}&mkey={c['mkey']}\"\n",
    "                drv.get(dl_url)\n",
    "            else:\n",
    "                drv.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", c[\"a\"])\n",
    "                drv.execute_script(\"arguments[0].click();\", c[\"a\"])\n",
    "            time.sleep(0.7)\n",
    "\n",
    "        # wait for downloads to finish\n",
    "        t0 = time.time()\n",
    "        max_wait = max(30, 15 * len(targets))\n",
    "        while time.time() - t0 < max_wait:\n",
    "            time.sleep(0.5)\n",
    "            if _downloads_done(out_dir):\n",
    "                break\n",
    "\n",
    "        after = {p.name for p in out_dir.glob(\"*\") if p.is_file()}\n",
    "        new_files = sorted(list(after - before))\n",
    "\n",
    "        # Update ledger with the doc_ids we attempted (only if new files appeared)\n",
    "        for c in targets:\n",
    "            if c[\"doc_id\"]:\n",
    "                ledger.add(c[\"doc_id\"])\n",
    "        _save_ledger(out_dir, ledger)\n",
    "\n",
    "        return {\"ok\": True, \"matched\": len(targets), \"files\": [str(out_dir / f) for f in new_files]}\n",
    "\n",
    "    finally:\n",
    "        drv.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44eeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, zipfile\n",
    "# from pathlib import Path\n",
    "# import requests\n",
    "# from urllib.parse import urlparse\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from urllib3.util.retry import Retry\n",
    "\n",
    "# def _safe_name(s: str) -> str:\n",
    "#     s = s.strip() or \"file\"\n",
    "#     return re.sub(r'[\\\\/*?:\"<>|]+', \"_\", s)\n",
    "\n",
    "# def _unique_path(base_dir: Path, name: str) -> Path:\n",
    "#     p = base_dir / name\n",
    "#     if not p.exists(): return p\n",
    "#     stem, ext = Path(name).stem, Path(name).suffix\n",
    "#     i = 1\n",
    "#     while True:\n",
    "#         q = base_dir / f\"{stem} ({i}){ext}\"\n",
    "#         if not q.exists(): return q\n",
    "#         i += 1\n",
    "\n",
    "# def download_pecas_flat(row, out_root=\"../docs\", timeout=60):\n",
    "#     out_dir = Path(out_root) / str(row['id'])\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # session with retries\n",
    "#     sess = requests.Session()\n",
    "#     sess.headers.update({\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "#         \"Referer\": \"https://www.acingov.pt/\",\n",
    "#         \"Accept\": \"*/*\",\n",
    "#     })\n",
    "#     sess.mount(\"https://\", HTTPAdapter(max_retries=Retry(total=3, backoff_factor=0.5,\n",
    "#                                                         status_forcelist=[429, 500, 502, 503, 504])))\n",
    "\n",
    "#     r = sess.get(row['link'], stream=True, timeout=timeout)\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     # best-effort filename from headers or URL tail\n",
    "#     fname = None\n",
    "#     cd = r.headers.get(\"Content-Disposition\", \"\")\n",
    "#     m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\";]+)\"?', cd)\n",
    "#     if m:\n",
    "#         fname = m.group(1)\n",
    "#     if not fname:\n",
    "#         tail = Path(urlparse(row['link']).path).name or \"pecas\"\n",
    "#         fname = tail\n",
    "#     if \".\" not in fname:\n",
    "#         ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "#         if \"zip\" in ctype: ext = \".zip\"\n",
    "#         elif \"pdf\" in ctype: ext = \".pdf\"\n",
    "#         elif \"jnlp\" in ctype: ext = \".jnlp\"\n",
    "#         else: ext = \".bin\"\n",
    "#         fname += ext\n",
    "#     fname = _safe_name(fname)\n",
    "\n",
    "#     # save to disk\n",
    "#     tmp_path = _unique_path(out_dir, fname)\n",
    "#     with open(tmp_path, \"wb\") as f:\n",
    "#         for chunk in r.iter_content(1024 * 64):\n",
    "#             if chunk: f.write(chunk)\n",
    "\n",
    "#     saved = []\n",
    "#     # If zip: extract flat into out_dir and delete the original zip\n",
    "#     if tmp_path.suffix.lower() == \".zip\":\n",
    "#         try:\n",
    "#             with zipfile.ZipFile(tmp_path, \"r\") as zf:\n",
    "#                 for info in zf.infolist():\n",
    "#                     if info.is_dir():\n",
    "#                         continue\n",
    "#                     # flatten: take only basename, drop internal folders\n",
    "#                     name = _safe_name(Path(info.filename).name)\n",
    "#                     dest = _unique_path(out_dir, name)\n",
    "#                     # extract to bytes, then write (avoids path traversal)\n",
    "#                     with zf.open(info, \"r\") as src, open(dest, \"wb\") as dst:\n",
    "#                         dst.write(src.read())\n",
    "#                     saved.append(str(dest))\n",
    "#         finally:\n",
    "#             # remove the original zip regardless of extraction result\n",
    "#             try: tmp_path.unlink()\n",
    "#             except Exception: pass\n",
    "#     else:\n",
    "#         saved.append(str(tmp_path))\n",
    "\n",
    "#     return {\"ok\": True, \"contract_id\": row['id'], \"files\": saved}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, zipfile, time\n",
    "# from pathlib import Path\n",
    "# import requests\n",
    "# from urllib.parse import urlparse, unquote\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from urllib3.util.retry import Retry\n",
    "\n",
    "# def _safe_name(s: str) -> str:\n",
    "#     s = s.strip() or \"file\"\n",
    "#     return re.sub(r'[\\\\/*?:\"<>|]+', \"_\", s)\n",
    "\n",
    "# def _unique_path(base_dir: Path, name: str) -> Path:\n",
    "#     p = base_dir / name\n",
    "#     if not p.exists(): return p\n",
    "#     stem, ext = Path(name).stem, Path(name).suffix\n",
    "#     i = 1\n",
    "#     while True:\n",
    "#         q = base_dir / f\"{stem} ({i}){ext}\"\n",
    "#         if not q.exists(): return q\n",
    "#         i += 1\n",
    "\n",
    "# def _make_session():\n",
    "#     sess = requests.Session()\n",
    "#     sess.headers.update({\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "#         \"Referer\": \"https://www.acingov.pt/\",\n",
    "#         \"Accept\": \"*/*\",\n",
    "#     })\n",
    "#     retries = Retry(\n",
    "#         total=5, connect=5, read=5,\n",
    "#         backoff_factor=0.6,\n",
    "#         status_forcelist=[429, 500, 502, 503, 504],\n",
    "#         allowed_methods={\"GET\", \"HEAD\", \"OPTIONS\"},\n",
    "#         respect_retry_after_header=True,\n",
    "#         raise_on_redirect=True\n",
    "#     )\n",
    "#     adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)\n",
    "#     sess.mount(\"https://\", adapter)\n",
    "#     sess.mount(\"http://\", adapter)\n",
    "#     return sess\n",
    "\n",
    "# def _pick_filename(resp, url_tail: str) -> str:\n",
    "#     # Try Content-Disposition (handles RFC5987 filename*=UTF-8'')\n",
    "#     cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
    "#     m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\";]+)\"?', cd)\n",
    "#     if m:\n",
    "#         fname = unquote(m.group(1))\n",
    "#     else:\n",
    "#         fname = Path(url_tail).name or \"pecas\"\n",
    "#     if \".\" not in fname:\n",
    "#         ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "#         if \"zip\" in ctype: ext = \".zip\"\n",
    "#         elif \"pdf\" in ctype: ext = \".pdf\"\n",
    "#         elif \"jnlp\" in ctype: ext = \".jnlp\"\n",
    "#         else: ext = \".bin\"\n",
    "#         fname += ext\n",
    "#     return _safe_name(fname)\n",
    "\n",
    "# def _download_with_resume(sess, url, out_path: Path,\n",
    "#                           chunk_size=64 * 1024,\n",
    "#                           connect_timeout=5,\n",
    "#                           read_timeout=180,\n",
    "#                           max_attempts=8,\n",
    "#                           max_total_time=None):\n",
    "#     \"\"\"\n",
    "#     Stream download that survives read timeouts by resuming with HTTP Range.\n",
    "#     \"\"\"\n",
    "#     out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     tmp_path = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "\n",
    "#     pos = tmp_path.stat().st_size if tmp_path.exists() else 0\n",
    "#     start_time = time.time()\n",
    "#     attempts = 0\n",
    "#     etag = None\n",
    "#     headers = {}\n",
    "\n",
    "#     # First request also used to pick filename when caller hasn't decided yet\n",
    "#     while True:\n",
    "#         if max_total_time and (time.time() - start_time) > max_total_time:\n",
    "#             raise TimeoutError(f\"Aborted after {max_total_time}s total time: {url}\")\n",
    "\n",
    "#         range_hdr = {\"Range\": f\"bytes={pos}-\"} if pos > 0 else {}\n",
    "#         resp = sess.get(url, stream=True, headers=range_hdr | headers,\n",
    "#                         timeout=(connect_timeout, read_timeout))\n",
    "\n",
    "#         # If we tried to resume but server ignored Range, restart from scratch\n",
    "#         if pos > 0 and resp.status_code == 200:\n",
    "#             pos = 0\n",
    "#             tmp_path.write_bytes(b\"\")\n",
    "\n",
    "#         # Capture ETag so we can protect resumes\n",
    "#         if etag is None and resp.headers.get(\"ETag\"):\n",
    "#             etag = resp.headers[\"ETag\"]\n",
    "#             headers[\"If-Range\"] = etag\n",
    "\n",
    "#         try:\n",
    "#             with tmp_path.open(\"ab\") as f:\n",
    "#                 for chunk in resp.iter_content(chunk_size):\n",
    "#                     if chunk:\n",
    "#                         f.write(chunk)\n",
    "#                         pos += len(chunk)\n",
    "#             break  # success\n",
    "#         except (requests.ReadTimeout, requests.ConnectionError, requests.ChunkedEncodingError) as e:\n",
    "#             attempts += 1\n",
    "#             resp.close()\n",
    "#             if attempts > max_attempts:\n",
    "#                 raise ConnectionError(f\"Exceeded retries while downloading {url}\") from e\n",
    "#             # small exponential backoff\n",
    "#             time.sleep(min(0.25 * (2 ** min(attempts, 6)), 10))\n",
    "#             continue\n",
    "#         finally:\n",
    "#             resp.close()\n",
    "\n",
    "#     tmp_path.replace(out_path)\n",
    "#     return out_path\n",
    "\n",
    "# def download_pecas_flat(row, out_root=\"../docs\", timeout=180):\n",
    "#     \"\"\"\n",
    "#     timeout can be:\n",
    "#       - int/float -> used as read timeout, connect timeout defaults to 5s\n",
    "#       - (connect_timeout, read_timeout) tuple\n",
    "#     \"\"\"\n",
    "#     out_dir = Path(out_root) / str(row['id'])\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # session with robust retries\n",
    "#     sess = _make_session()\n",
    "\n",
    "#     url = row['link']\n",
    "#     connect_t, read_t = (5, timeout) if not isinstance(timeout, (tuple, list)) else tuple(timeout)\n",
    "\n",
    "#     # HEAD (optional) – helps with early filename detection & quick failure\n",
    "#     try:\n",
    "#         h = sess.head(url, allow_redirects=True, timeout=(connect_t, min(read_t, 10)))\n",
    "#         h.raise_for_status()\n",
    "#         fname = _pick_filename(h, urlparse(url).path)\n",
    "#     except Exception:\n",
    "#         # fallback to GET-derived name later\n",
    "#         fname = None\n",
    "\n",
    "#     # If we didn't get a name from HEAD, use a short GET to decide name\n",
    "#     if not fname:\n",
    "#         g = sess.get(url, stream=True, timeout=(connect_t, min(read_t, 15)))\n",
    "#         g.raise_for_status()\n",
    "#         fname = _pick_filename(g, urlparse(url).path)\n",
    "#         g.close()\n",
    "\n",
    "#     fname = _safe_name(fname)\n",
    "#     out_path = _unique_path(out_dir, fname)\n",
    "\n",
    "#     # Do the resilient download (with resume on stall)\n",
    "#     _download_with_resume(\n",
    "#         sess, url, out_path,\n",
    "#         chunk_size=64 * 1024,\n",
    "#         connect_timeout=connect_t,\n",
    "#         read_timeout=read_t,\n",
    "#         max_attempts=8,\n",
    "#         max_total_time=None,   # or set a ceiling (e.g., 30*60)\n",
    "#     )\n",
    "\n",
    "#     saved = []\n",
    "\n",
    "#     # If zip: extract flat into out_dir and delete the original zip\n",
    "#     if out_path.suffix.lower() == \".zip\":\n",
    "#         try:\n",
    "#             with zipfile.ZipFile(out_path, \"r\") as zf:\n",
    "#                 for info in zf.infolist():\n",
    "#                     if info.is_dir():\n",
    "#                         continue\n",
    "#                     name = _safe_name(Path(info.filename).name)\n",
    "#                     dest = _unique_path(out_dir, name)\n",
    "#                     # read -> write to prevent path traversal\n",
    "#                     with zf.open(info, \"r\") as src, open(dest, \"wb\") as dst:\n",
    "#                         dst.write(src.read())\n",
    "#                     saved.append(str(dest))\n",
    "#         finally:\n",
    "#             try: out_path.unlink()\n",
    "#             except Exception: pass\n",
    "#     else:\n",
    "#         saved.append(str(out_path))\n",
    "\n",
    "#     return {\"ok\": True, \"contract_id\": row['id'], \"files\": saved}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io, zipfile\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# def _extract_zip_flat(zip_path: Path, out_dir: Path,\n",
    "#                       max_depth: int = 2,\n",
    "#                       depth: int = 0,\n",
    "#                       max_files: int = 5000,\n",
    "#                       max_uncompressed_bytes: int = 2_000_000_000):\n",
    "#     \"\"\"\n",
    "#     Extracts `zip_path` into `out_dir`, flattening paths (keeps only base file names),\n",
    "#     and recursively extracts nested zips up to `max_depth`.\n",
    "\n",
    "#     Safety:\n",
    "#       - Avoids Zip Slip by ignoring inner directories and using only basename.\n",
    "#       - Caps total files and total uncompressed size (best-effort).\n",
    "#       - Limits recursion depth to avoid endless nesting.\n",
    "\n",
    "#     Returns: list of saved file paths (strings).\n",
    "#     \"\"\"\n",
    "#     saved = []\n",
    "#     file_count = 0\n",
    "#     total_uncompressed = 0\n",
    "\n",
    "#     def _extract_stream(zf: zipfile.ZipFile, out_dir: Path, depth: int):\n",
    "#         nonlocal file_count, total_uncompressed, saved\n",
    "\n",
    "#         for info in zf.infolist():\n",
    "#             if info.is_dir():\n",
    "#                 continue\n",
    "\n",
    "#             # zip-bomb guards\n",
    "#             total_uncompressed += info.file_size\n",
    "#             file_count += 1\n",
    "#             if file_count > max_files:\n",
    "#                 raise RuntimeError(f\"Aborting: too many files ({file_count}>{max_files}) in nested zips.\")\n",
    "#             if total_uncompressed > max_uncompressed_bytes:\n",
    "#                 raise RuntimeError(f\"Aborting: uncompressed size limit exceeded ({total_uncompressed} bytes).\")\n",
    "\n",
    "#             name = _safe_name(Path(info.filename).name)\n",
    "#             dest = _unique_path(out_dir, name)\n",
    "\n",
    "#             with zf.open(info, \"r\") as src:\n",
    "#                 data = src.read()\n",
    "\n",
    "#             # nested zip?\n",
    "#             if name.lower().endswith(\".zip\") and depth < max_depth:\n",
    "#                 try:\n",
    "#                     with zipfile.ZipFile(io.BytesIO(data), \"r\") as nested:\n",
    "#                         _extract_stream(nested, out_dir, depth + 1)\n",
    "#                     # don't store the intermediate .zip if we could unpack it\n",
    "#                     continue\n",
    "#                 except zipfile.BadZipFile:\n",
    "#                     # not actually a valid zip; fall through and write bytes\n",
    "#                     pass\n",
    "\n",
    "#             with open(dest, \"wb\") as f:\n",
    "#                 f.write(data)\n",
    "#             saved.append(str(dest))\n",
    "\n",
    "#     with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "#         _extract_stream(zf, out_dir, depth)\n",
    "\n",
    "#     # remove the original .zip after successful extraction\n",
    "#     try:\n",
    "#         zip_path.unlink()\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     return saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeabdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, zipfile, time\n",
    "# from pathlib import Path\n",
    "# import requests\n",
    "# from urllib.parse import urlparse, unquote\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from urllib3.util.retry import Retry\n",
    "\n",
    "# def _safe_name(s: str) -> str:\n",
    "#     s = s.strip() or \"file\"\n",
    "#     return re.sub(r'[\\\\/*?:\"<>|]+', \"_\", s)\n",
    "\n",
    "# def _unique_path(base_dir: Path, name: str) -> Path:\n",
    "#     p = base_dir / name\n",
    "#     if not p.exists(): return p\n",
    "#     stem, ext = Path(name).stem, Path(name).suffix\n",
    "#     i = 1\n",
    "#     while True:\n",
    "#         q = base_dir / f\"{stem} ({i}){ext}\"\n",
    "#         if not q.exists(): return q\n",
    "#         i += 1\n",
    "\n",
    "# def _make_session():\n",
    "#     sess = requests.Session()\n",
    "#     sess.headers.update({\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "#         \"Referer\": \"https://www.acingov.pt/\",\n",
    "#         \"Accept\": \"*/*\",\n",
    "#     })\n",
    "#     retries = Retry(\n",
    "#         total=5, connect=5, read=5,\n",
    "#         backoff_factor=0.6,\n",
    "#         status_forcelist=[429, 500, 502, 503, 504],\n",
    "#         allowed_methods={\"GET\", \"HEAD\", \"OPTIONS\"},\n",
    "#         respect_retry_after_header=True,\n",
    "#         raise_on_redirect=True\n",
    "#     )\n",
    "#     adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)\n",
    "#     sess.mount(\"https://\", adapter)\n",
    "#     sess.mount(\"http://\", adapter)\n",
    "#     return sess\n",
    "\n",
    "# def _pick_filename(resp, url_tail: str) -> str:\n",
    "#     # Try Content-Disposition (handles RFC5987 filename*=UTF-8'')\n",
    "#     cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
    "#     m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\";]+)\"?', cd)\n",
    "#     if m:\n",
    "#         fname = unquote(m.group(1))\n",
    "#     else:\n",
    "#         fname = Path(url_tail).name or \"pecas\"\n",
    "#     if \".\" not in fname:\n",
    "#         ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "#         if \"zip\" in ctype: ext = \".zip\"\n",
    "#         elif \"pdf\" in ctype: ext = \".pdf\"\n",
    "#         elif \"jnlp\" in ctype: ext = \".jnlp\"\n",
    "#         else: ext = \".bin\"\n",
    "#         fname += ext\n",
    "#     return _safe_name(fname)\n",
    "\n",
    "# def _download_with_resume(sess, url, out_path: Path,\n",
    "#                           chunk_size=64 * 1024,\n",
    "#                           connect_timeout=5,\n",
    "#                           read_timeout=180,\n",
    "#                           max_attempts=8,\n",
    "#                           max_total_time=None):\n",
    "#     \"\"\"\n",
    "#     Stream download that survives read timeouts by resuming with HTTP Range.\n",
    "#     \"\"\"\n",
    "#     out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     tmp_path = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "\n",
    "#     pos = tmp_path.stat().st_size if tmp_path.exists() else 0\n",
    "#     start_time = time.time()\n",
    "#     attempts = 0\n",
    "#     etag = None\n",
    "#     headers = {}\n",
    "\n",
    "#     # First request also used to pick filename when caller hasn't decided yet\n",
    "#     while True:\n",
    "#         if max_total_time and (time.time() - start_time) > max_total_time:\n",
    "#             raise TimeoutError(f\"Aborted after {max_total_time}s total time: {url}\")\n",
    "\n",
    "#         range_hdr = {\"Range\": f\"bytes={pos}-\"} if pos > 0 else {}\n",
    "#         resp = sess.get(url, stream=True, headers=range_hdr | headers,\n",
    "#                         timeout=(connect_timeout, read_timeout))\n",
    "\n",
    "#         # If we tried to resume but server ignored Range, restart from scratch\n",
    "#         if pos > 0 and resp.status_code == 200:\n",
    "#             pos = 0\n",
    "#             tmp_path.write_bytes(b\"\")\n",
    "\n",
    "#         # Capture ETag so we can protect resumes\n",
    "#         if etag is None and resp.headers.get(\"ETag\"):\n",
    "#             etag = resp.headers[\"ETag\"]\n",
    "#             headers[\"If-Range\"] = etag\n",
    "\n",
    "#         try:\n",
    "#             with tmp_path.open(\"ab\") as f:\n",
    "#                 for chunk in resp.iter_content(chunk_size):\n",
    "#                     if chunk:\n",
    "#                         f.write(chunk)\n",
    "#                         pos += len(chunk)\n",
    "#             break  # success\n",
    "#         except (requests.ReadTimeout, requests.ConnectionError, requests.ChunkedEncodingError) as e:\n",
    "#             attempts += 1\n",
    "#             resp.close()\n",
    "#             if attempts > max_attempts:\n",
    "#                 raise ConnectionError(f\"Exceeded retries while downloading {url}\") from e\n",
    "#             # small exponential backoff\n",
    "#             time.sleep(min(0.25 * (2 ** min(attempts, 6)), 10))\n",
    "#             continue\n",
    "#         finally:\n",
    "#             resp.close()\n",
    "\n",
    "#     tmp_path.replace(out_path)\n",
    "#     return out_path\n",
    "\n",
    "# def download_pecas_flat(row, out_root=\"../docs\", timeout=180):\n",
    "#     \"\"\"\n",
    "#     timeout can be:\n",
    "#       - int/float -> used as read timeout, connect timeout defaults to 5s\n",
    "#       - (connect_timeout, read_timeout) tuple\n",
    "#     \"\"\"\n",
    "#     out_dir = Path(out_root) / str(row['id'])\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # session with robust retries\n",
    "#     sess = _make_session()\n",
    "\n",
    "#     url = row['link']\n",
    "#     connect_t, read_t = (5, timeout) if not isinstance(timeout, (tuple, list)) else tuple(timeout)\n",
    "\n",
    "#     # HEAD (optional) – helps with early filename detection & quick failure\n",
    "#     try:\n",
    "#         h = sess.head(url, allow_redirects=True, timeout=(connect_t, min(read_t, 10)))\n",
    "#         h.raise_for_status()\n",
    "#         fname = _pick_filename(h, urlparse(url).path)\n",
    "#     except Exception:\n",
    "#         # fallback to GET-derived name later\n",
    "#         fname = None\n",
    "\n",
    "#     # If we didn't get a name from HEAD, use a short GET to decide name\n",
    "#     if not fname:\n",
    "#         g = sess.get(url, stream=True, timeout=(connect_t, min(read_t, 15)))\n",
    "#         g.raise_for_status()\n",
    "#         fname = _pick_filename(g, urlparse(url).path)\n",
    "#         g.close()\n",
    "\n",
    "#     fname = _safe_name(fname)\n",
    "#     out_path = _unique_path(out_dir, fname)\n",
    "\n",
    "#     # Do the resilient download (with resume on stall)\n",
    "#     _download_with_resume(\n",
    "#         sess, url, out_path,\n",
    "#         chunk_size=64 * 1024,\n",
    "#         connect_timeout=connect_t,\n",
    "#         read_timeout=read_t,\n",
    "#         max_attempts=8,\n",
    "#         max_total_time=None,   # or set a ceiling (e.g., 30*60)\n",
    "#     )\n",
    "\n",
    "#     saved = []\n",
    "\n",
    "#     # If zip: extract flat into out_dir and delete the original zip\n",
    "#     if out_path.suffix.lower() == \".zip\":\n",
    "#         try:\n",
    "#             saved = _extract_zip_flat(out_path, out_dir, max_depth=3)\n",
    "#         except Exception as e:\n",
    "#             # If it fails, keep the original zip so you can inspect it\n",
    "#             saved = [str(out_path)]\n",
    "#     else:\n",
    "#         saved = [str(out_path)]\n",
    "\n",
    "#     return {\"ok\": True, \"contract_id\": row['id'], \"files\": saved}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fa2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      {'ok': True, 'contract_id': 9722182, 'files': ...\n",
       "16     {'ok': True, 'contract_id': 9946477, 'files': ...\n",
       "26     {'ok': True, 'contract_id': 10020105, 'files':...\n",
       "27     {'ok': True, 'contract_id': 10020164, 'files':...\n",
       "28     {'ok': True, 'contract_id': 10034714, 'files':...\n",
       "                             ...                        \n",
       "783    {'ok': True, 'contract_id': 9980931, 'files': ...\n",
       "787    {'ok': True, 'contract_id': 9980868, 'files': ...\n",
       "790    {'ok': True, 'contract_id': 9694357, 'files': ...\n",
       "793    {'ok': True, 'contract_id': 9694859, 'files': ...\n",
       "796    {'ok': True, 'contract_id': 9980968, 'files': ...\n",
       "Length: 320, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contracts_download[:2].apply(download_pecas_flat, axis=1, out_root=\"../docs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskguard (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
